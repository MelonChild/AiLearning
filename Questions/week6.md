## 思考题   
> 为什么BERT在第一句前会加一个[CLS]标志?        
+ bert模型的输入可以是一个句子或者句子对，代码层面来说，就是输入了句子或者句子对对应的3个向量
+ token embedding segment embedding position embedding
+ 第一句前会加一个[CLS]标志,这个属于无明显语义信息的符号
+ 这个会更好的融合文本重的各个词的语义信息，从而更好的表示整句话的语义
+ 最后一层该位对应向量可以作为整句话的语义表示，从而用于下游的分类任务

  
> BERT的训练任务有几个，具体描述下每个任务的内容？  
+ BERT旨在通过联合调节所有层中的上下文来预先训练深度双向表示
+ BERT算法的原理由两部分组成
  + 通过对大量未标注的语料进行非监督的预训练，来学习其中的表达法
  + 使用少量标记的训练数据以监督方式微调预训练模型以进行各种监督任务。
+ 主要分类为四大基本任务：  
  + 句子对分类任务

    例如预测下一句、语义相似度等任务，输入是两个句子，中间用哪个[SEP]分隔，最终得到的class label 就表示是否下一句或者是否是语义相似

  + 单句分类任务：

    常见的文本分类、情感分析等，输入的就是一个单独的句子，最终的 class label就是表示句子是属于哪一种类型，或者句子包含的什么样的情感。
  + 问答任务：
  
    主要用于SQuAD数据集，输入是一个问题和问题对应的段落，用[SEP]分隔，输出的结果就是答案在给定段落的开始和终止位置，主要用于阅读理解任务。
  + 单句子标注：NER 
    
    命名实体识别任务，常见的数据集有CoNLL-2003 NER[32].识别文本中具有特定意义的实体，主要包括人名、地名、机构名、专有名词等，以及时间、数量、货币、比例数值等文字。


> 如何利用BERT等预训练模型完成抽提式文本摘要任务？
+ 单文档文本摘要是自动生成文档的较短版本，同时保留其最重要信息的任务
+ 在输入层将文本原文按句子分开
+ 句子中间插入【SEP】和【CLS】输入
+ 在Interval Segment Embeddings中将句子分别依次设置成EA和EB，最终训练整个模型
+ 在output层拿到【CLS】的状态表征，再接上一层全连接层、RNN、也可以再加一层Transformer
+ 一个句子是最终文本摘要的结果，那么这个句子输出标签为1，以此方法实现抽提式文本摘要的任务

 
> BERT有哪些可以提升的地方呢？ 
+ 训练过程中随机选择 15% 的词汇用于预测，这样每批只有15%的标记被预测，这个值是否可调，或者针对它来优化减小训练步骤，进而减少计算呢

