# Homework-week4
2020-06-13 基于Transformer特征 提取器的改进

## GitHub

    https://github.com/MelonChild/AiLearning    
    
    tag: T-Week-5

## 目录文件说明

+ questions
  
  思考题

## 要求

### week5

- 不考虑多头的原因，self-attention中词向量不乘QKV（Wq、Wk、Wv）参数矩阵，会有什么问题？

- Transformer的点积模型做缩放的原因是什么？

- Self-Attention 的时间复杂度是怎么计算的？为多少？

- 根据问题3求的计算复杂度可以看出，输入序列长度过长会造成计算量太大，那你有什么的想法从结构上改进么？

## 更新日志

### questions

基于Transformer特征 提取器的改进

+ 2020-06-15

  回顾课程回答问题
