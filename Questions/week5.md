

<!--
 * @Author       : IdealCoder
 * @Date         : 2020-06-15 09:32
 * @LastEditTime : 2020-06-18 17:04
 * @Description  : 
 * @FilePath     : /AiLearning/Questions/week5.md
 * @
--> 

## 思考题   
> 不考虑多头的原因，self-attention中词向量不乘QKV（Wq、Wk、Wv）参数矩阵，会有什么问题?        
+ QKV参数分别是：Q:query ,K:key，V:value，均来自统一输入
+ 首先计算Q于K之间的点乘，用来判断当前词与所有词的相关度分数    
+ 然后用attention score与V相乘，得到加权向量
+ 最终输出的是经过加权后的向量，所以每个词都会带有与其他词关联的信息，也可以增加位置信息，即更好的利用了上下文的相关信息
+ 如果不加这些，在RNN中，如果两个词相距很远的话，效果会很差
+ Wq\Wk\Wv 是embedding维度*dk，作用就是区别QKV,使其不一致，如果不使用参数矩阵，那么Q K完全一致，这样点乘之后，当前词本省的score会很高，经过softmax,其余词和当前词比重很小，很难较好的注意到其他词汇
  
> Transformer的点积模型做缩放的原因是什么?  
+ query key 进行一次线性变换分别映射到dk维
+ dk比较大的情况下，点乘的结果会很大，这样经过softmax函数，其值会落在梯度很小的一段，不利于训练，所以需要对结果进行缩放以获取到更稳定的区间
+ dk = dv=d(model)/h 

> Self-Attention 的时间复杂度是怎么计算的？为多少？ 
+ 实现过程中，计算会以矩阵的形式完成，以便更快的处理
+ 矩阵化的计算和单个向量计算过程相同
+ 如果当前是n个词向量,d维的参数矩阵进行计算，：
  1. 计算相似度，即QK的点乘，Q*K^T 即（n,d)*(d,n) = O(n*n*d) 
  2. 获得的(n,n)矩阵进行softmax计算 O(n*n)
  3. 然后求加权后的Z 相当于 (n,n)*(n,d) = O(n*d*n)
  4. 因此，Self-Attention的时间复杂度为：O(n^2*d)
 
> 根据问题3求的计算复杂度可以看出，输入序列长度过长会造成计算量太大，那你有什么的想法从结构上改进么？   
+ 根据复杂度计算的第一步，也就是相似度计算可以看出，相当于是每一个词和所有词都进行了计算，但其实，当前句子中有些词的关联关系很小，可以忽略不计，这样就能少计算一些数据
+ 另外 矩阵的维度d 也很关键，可以选择合适的维度降低复杂度

